{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from torch_cluster import random_walk\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "# max_sequence length\n",
    "# num_class\n",
    "# vocab_size\n",
    "batch_size = 1\n",
    "walk_length = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "path = '../data/ENZYMES/'\n",
    "dataset = TUDataset(path, name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "n = len(dataset) // 10\n",
    "\n",
    "test_dataset = dataset[:n]\n",
    "train_dataset = dataset[n:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_loader.__iter__().__next__()\n",
    "x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  ..., 31, 31, 31])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, k, num_heads=8):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # determine queries, keys, values\n",
    "        self.key_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.query_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.value_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        \n",
    "        # project down all cat-ed heads\n",
    "        self.unify_layer = nn.Linear(self.num_heads * k, k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get batch size, t sentences of k items\n",
    "        b_sz, t_sz, k_sz = x.size()\n",
    "        h_sz = self.num_heads\n",
    "        \n",
    "        keys = self.key_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        queries = self.query_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        values = self.value_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "    \n",
    "        # compute dot products (k x k). Same op for every head, so fold in to the\n",
    "        # batch dim\n",
    "        # q, k, v, (b, t, h, k) -> (b, h, t, k) -> (bh, t, k)\n",
    "        # and for the key (bh, t, k) -> (bh, k, t) to be able to use bmm\n",
    "        #\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        values = values.transpose(1, 2).contiguous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # intermediate scaling\n",
    "        queries = queries / ( self.k  ** (1./4.))\n",
    "        keys = keys / ( self.k  ** (1./4.))\n",
    "        \n",
    "        # final transpose for the bmm, out -> (b*h, t, t)\n",
    "        raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        \n",
    "        # row wise softmax normalize\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        \n",
    "        # apply self attention to the values\n",
    "        out = torch.bmm(weights, values).view(b_sz, h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # Unify attention heads\n",
    "        # reshuffle (b, h, t, k) -> (b, t, h, k) -> (b, t, h*k) with all the heads catted\n",
    "        # ontop of each other to be able to down project\n",
    "        out = out.transpose(1, 2).contiguous().view(b_sz, t_sz, h_sz * k_sz)\n",
    "        \n",
    "        # project down\n",
    "        out = self.unify_layer(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = AttentionLayer(k, num_heads)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(k)\n",
    "        self.layer_norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(k, 4 * k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * k, k)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        x_att = self.attention(x)\n",
    "        # Residual + norm\n",
    "        x = self.layer_norm1(x + x_att)\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "        out = self.layer_norm2(x + x_mlp)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, k, num_heads, depth, num_tokens, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        # Embedding tokens and position layers\n",
    "        self.token_embed_layer = nn.Embedding(num_tokens, k)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.tf_network = []\n",
    "        for _ in range(depth):\n",
    "            self.tf_network.append(TransformerBlock(k, num_heads))\n",
    "\n",
    "        self.tf_network = nn.Sequential(*self.tf_network)\n",
    "        \n",
    "        # Sequence to class output\n",
    "        self.output_layer = nn.Linear(k, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in (b, t) tensor with int values representing words\n",
    "        # out (b, c) tensor logprobs over c classes\n",
    "        \n",
    "        # generate token embeddings\n",
    "        tokens = self.token_embed_layer(x)\n",
    "        \n",
    "        b_sz, t_sz, k_sz = tokens.size()\n",
    "        \n",
    "        # Transformer forward\n",
    "        x = self.tf_network(tokens)\n",
    "        \n",
    "        # Average pool over t dimension and project to class probabilities\n",
    "        x = self.output_layer(x.mean(dim=1))\n",
    "        \n",
    "        # Optional (auto-regressive) transformer\n",
    "        # no looking ahead, enforce via mask, prior to softmax\n",
    "#         indices = torch.triu_indices(t, t, offset=1)\n",
    "#         x[:, indices[0], indices[1]] = float('-inf')\n",
    "        \n",
    "        out = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_class):\n",
    "\n",
    "    # Ensure labels are [N x 1]\n",
    "    if len(list(labels.size())) == 1:\n",
    "        labels = labels.unsqueeze(1)\n",
    "\n",
    "    mask = torch.DoubleTensor(labels.size(0), n_class).fill_(0)\n",
    "\n",
    "    # scatter dimension, position indices, fill_value\n",
    "    return mask.scatter_(1, labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "num_classes = 6\n",
    "vocab_size = 128\n",
    "num_heads = 8\n",
    "depth = 6\n",
    "p, q = 1, 1\n",
    "num_epochs = 16\n",
    "# k, num_heads, depth, seq_length, num_tokens, num_\n",
    "model = Transformer(EMBED_DIM, num_heads, depth, vocab_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_warmup = 10000\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "opt = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (lr_warmup / batch_size), 1.0))\n",
    "loss_func = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print('epoch: {}'.format(epoch))\n",
    "    for graph in train_dataset:\n",
    "        x, edge_index, y = graph.x, graph.edge_index, graph.y\n",
    "        subset = torch.arange(x.size(0), device=edge_index.device)\n",
    "        walks = random_walk(edge_index[0], edge_index[1], subset, walk_length, p, q, x.size(0))\n",
    "        # model forward\n",
    "        y = y.repeat(walks.size(0),)\n",
    "        y_pred = model(walks)\n",
    "        \n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss_hist.append(loss.item())\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        sch.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
