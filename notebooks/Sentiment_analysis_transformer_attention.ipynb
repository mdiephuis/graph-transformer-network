{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with an attention transformer\n",
    "* sentiment bit adapted from: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "* transformer from: bloem\n",
    "* data loader copy-paste from https://github.com/ben0it8/containerized-transformer-finetuning/blob/develop/research/finetune-transformer-on-imdb5k.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import gzip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for converting between nats and bits\n",
    "LOG2E = math.log2(math.e)\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "NUM_CLS = 2\n",
    "vocab_size = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train, test = tdata.split(split_ratio=0.8)\n",
    "\n",
    "TEXT.build_vocab(train, max_size=vocab_size - 2) # - 2 to make space for <unk> and <pad>\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=4, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- nr. of training examples 5000\n",
      "- nr. of validation examples 1250\n"
     ]
    }
   ],
   "source": [
    "print(f'- nr. of training examples {len(train_iter)}')\n",
    "print(f'- nr. of validation examples {len(test_iter)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- maximum sequence length: 4940\n"
     ]
    }
   ],
   "source": [
    "mx = max([input.text[0].size(1) for input in train_iter])\n",
    "mx = mx * 2\n",
    "print(f'- maximum sequence length: {mx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, k, num_heads=8):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # determine queries, keys, values\n",
    "        self.key_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.query_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        self.value_layer = nn.Linear(self.k, self.k * self.num_heads, bias=False)\n",
    "        \n",
    "        # project down all cat-ed heads\n",
    "        self.unify_layer = nn.Linear(self.num_heads * k, k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get batch size, t sentences of k items\n",
    "        b_sz, t_sz, k_sz = x.size()\n",
    "        h_sz = self.num_heads\n",
    "        \n",
    "        keys = self.key_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        queries = self.query_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "        values = self.value_layer(x).view(b_sz, t_sz, h_sz, self.k)\n",
    "    \n",
    "        # compute dot products (k x k). Same op for every head, so fold in to the\n",
    "        # batch dim\n",
    "        # q, k, v, (b, t, h, k) -> (b, h, t, k) -> (bh, t, k)\n",
    "        # and for the key (bh, t, k) -> (bh, k, t) to be able to use bmm\n",
    "        #\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        values = values.transpose(1, 2).contiguous().view(b_sz * h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # intermediate scaling\n",
    "        queries = queries / ( self.k  ** (1./4.))\n",
    "        keys = keys / ( self.k  ** (1./4.))\n",
    "        \n",
    "        # final transpose for the bmm, out -> (b*h, t, t)\n",
    "        raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        \n",
    "        # row wise softmax normalize\n",
    "        weights = F.softmax(raw_weights, dim=2)\n",
    "        \n",
    "        # apply self attention to the values\n",
    "        out = torch.bmm(weights, values).view(b_sz, h_sz, t_sz, k_sz)\n",
    "        \n",
    "        # Unify attention heads\n",
    "        # reshuffle (b, h, t, k) -> (b, t, h, k) -> (b, t, h*k) with all the heads catted\n",
    "        # ontop of each other to be able to down project\n",
    "        out = out.transpose(1, 2).contiguous().view(b_sz, t_sz, h_sz * k_sz)\n",
    "        \n",
    "        # project down\n",
    "        out = self.unify_layer(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, k, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = AttentionLayer(k, num_heads)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(k)\n",
    "        self.layer_norm2 = nn.LayerNorm(k)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(k, 4 * k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * k, k)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Attention block\n",
    "        x_att = self.attention(x)\n",
    "        # Residual + norm\n",
    "        x = self.layer_norm1(x + x_att)\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "        out = self.layer_norm2(x + x_mlp)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, k, num_heads, depth, num_tokens, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        # Embedding tokens and position layers\n",
    "        self.token_embed_layer = nn.Embedding(num_tokens, k)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.tf_network = []\n",
    "        for _ in range(depth):\n",
    "            self.tf_network.append(TransformerBlock(k, num_heads))\n",
    "\n",
    "        self.tf_network = nn.Sequential(*self.tf_network)\n",
    "        \n",
    "        # Sequence to class output\n",
    "        self.output_layer = nn.Linear(k, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # in (b, t) tensor with int values representing words\n",
    "        # out (b, c) tensor logprobs over c classes\n",
    "        \n",
    "        # generate token embeddings\n",
    "        tokens = self.token_embed_layer(x)\n",
    "        \n",
    "        b_sz, t_sz, k_sz = tokens.size()\n",
    "        \n",
    "        # Transformer forward\n",
    "        x = self.tf_network(tokens)\n",
    "        \n",
    "        # Average pool over t dimension and project to class probabilities\n",
    "        x = self.output_layer(x.mean(dim=1))\n",
    "        \n",
    "        # Optional (auto-regressive) transformer\n",
    "        # no looking ahead, enforce via mask, prior to softmax\n",
    "#         indices = torch.triu_indices(t, t, offset=1)\n",
    "#         x[:, indices[0], indices[1]] = float('-inf')\n",
    "        \n",
    "        out = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 128\n",
    "NUM_CLS = 2\n",
    "vocab_size = 50000\n",
    "num_heads = 8\n",
    "depth = 6\n",
    "# k, num_heads, depth, seq_length, num_tokens, num_\n",
    "model = Transformer(EMBED_DIM, num_heads, depth, vocab_size, NUM_CLS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_warmup = 10000\n",
    "batch_size = 4\n",
    "lr = 1e-3\n",
    "opt = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (lr_warmup / batch_size), 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main epoch transformer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maurits/Development/virtual_env/ML/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437b859f258a4d73bb1bbb2575f883f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-07ab7f0122d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/virtual_env/ML/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/virtual_env/ML/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(1):\n",
    "\n",
    "    print(f'\\n epoch {e}')\n",
    "    model.train(True)\n",
    "\n",
    "    for batch in tqdm(train_iter):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        input = batch.text[0]\n",
    "        label = batch.label - 1\n",
    "\n",
    "        if input.size(1) > mx:\n",
    "            input = input[:, :mx]\n",
    "        out = model(input)\n",
    "        loss = nn.functional.nll_loss(out, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # clip gradients\n",
    "        # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "#         if arg.gradient_clipping > 0.0:\n",
    "#             nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n",
    "\n",
    "        opt.step()\n",
    "        sch.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.train(False)\n",
    "        tot, cor= 0.0, 0.0\n",
    "\n",
    "        for batch in test_iter:\n",
    "\n",
    "            input = batch.text[0]\n",
    "            label = batch.label - 1\n",
    "\n",
    "            if input.size(1) > mx:\n",
    "                input = input[:, :mx]\n",
    "            out = model(input).argmax(dim=1)\n",
    "\n",
    "            tot += float(input.size(0))\n",
    "            cor += float((label == out).sum().item())\n",
    "\n",
    "        acc = cor / tot\n",
    "        print(f'-- {\"validation\"} accuracy {acc:.3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add position back\n",
    "# run on gallager\n",
    "# graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
